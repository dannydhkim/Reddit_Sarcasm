{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import Model\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, Embedding, Dense, Dropout, LSTM, Input, Concatenate, SpatialDropout1D\n",
    "#from keras import Sequential\n",
    "#from tensorflow.keras.layers import Embedding, Input, Dense, Flatten, Dropout, Bidirectional, LSTM\n",
    "#from keras.layers import  CuDNNLSTM\n",
    "#from keras.layers.merge import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ],
   "source": [
    "# First checking if GPU is available\n",
    "train_on_gpu=tf.test.is_gpu_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import random\n",
    "random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('C:/Users/Donghyeok/Desktop/Work Folder/Reddit Sarcasm Detection/train-balanced-sarcasm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('C:/Users/Donghyeok/Desktop/Work Folder/Reddit Sarcasm Detection/test-balanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "      <td>Trumpbart</td>\n",
       "      <td>politics</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-16 23:55:23</td>\n",
       "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>Shbshb906</td>\n",
       "      <td>nba</td>\n",
       "      <td>-4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>2016-11-01 00:24:10</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>Creepeth</td>\n",
       "      <td>nfl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-22 21:45:37</td>\n",
       "      <td>They're favored to win.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>icebrotha</td>\n",
       "      <td>BlackPeopleTwitter</td>\n",
       "      <td>-8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-18 21:03:47</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>cush2push</td>\n",
       "      <td>MaddenUltimateTeam</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>2016-12-30 17:00:13</td>\n",
       "      <td>Yep can confirm I saw the tool they use for th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010821</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm sure that Iran and N. Korea have the techn...</td>\n",
       "      <td>TwarkMain</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-04</td>\n",
       "      <td>2009-04-25 00:47:52</td>\n",
       "      <td>No one is calling this an engineered pathogen,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010822</th>\n",
       "      <td>1</td>\n",
       "      <td>whatever you do, don't vote green!</td>\n",
       "      <td>BCHarvey</td>\n",
       "      <td>climate</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-05</td>\n",
       "      <td>2009-05-14 22:27:40</td>\n",
       "      <td>In a move typical of their recent do-nothing a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010823</th>\n",
       "      <td>1</td>\n",
       "      <td>Perhaps this is an atheist conspiracy to make ...</td>\n",
       "      <td>rebelcommander</td>\n",
       "      <td>atheism</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-01</td>\n",
       "      <td>2009-01-11 00:22:57</td>\n",
       "      <td>Screw the Disabled--I've got to get to Church ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010824</th>\n",
       "      <td>1</td>\n",
       "      <td>The Slavs got their own country - it is called...</td>\n",
       "      <td>catsi</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-01</td>\n",
       "      <td>2009-01-23 21:12:49</td>\n",
       "      <td>I've always been unsettled by that. I hear a l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010825</th>\n",
       "      <td>1</td>\n",
       "      <td>values, as in capitalism .. there is good mone...</td>\n",
       "      <td>frogking</td>\n",
       "      <td>politics</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-01</td>\n",
       "      <td>2009-01-24 06:20:14</td>\n",
       "      <td>Why do the people who make our laws seem unabl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1010826 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                            comment  \\\n",
       "0            0                                         NC and NH.   \n",
       "1            0  You do know west teams play against west teams...   \n",
       "2            0  They were underdogs earlier today, but since G...   \n",
       "3            0  This meme isn't funny none of the \"new york ni...   \n",
       "4            0                    I could use one of those tools.   \n",
       "...        ...                                                ...   \n",
       "1010821      1  I'm sure that Iran and N. Korea have the techn...   \n",
       "1010822      1                 whatever you do, don't vote green!   \n",
       "1010823      1  Perhaps this is an atheist conspiracy to make ...   \n",
       "1010824      1  The Slavs got their own country - it is called...   \n",
       "1010825      1  values, as in capitalism .. there is good mone...   \n",
       "\n",
       "                 author           subreddit  score  ups  downs     date  \\\n",
       "0             Trumpbart            politics      2   -1     -1  2016-10   \n",
       "1             Shbshb906                 nba     -4   -1     -1  2016-11   \n",
       "2              Creepeth                 nfl      3    3      0  2016-09   \n",
       "3             icebrotha  BlackPeopleTwitter     -8   -1     -1  2016-10   \n",
       "4             cush2push  MaddenUltimateTeam      6   -1     -1  2016-12   \n",
       "...                 ...                 ...    ...  ...    ...      ...   \n",
       "1010821       TwarkMain          reddit.com      2    2      0  2009-04   \n",
       "1010822        BCHarvey             climate      1    1      0  2009-05   \n",
       "1010823  rebelcommander             atheism      1    1      0  2009-01   \n",
       "1010824           catsi           worldnews      1    1      0  2009-01   \n",
       "1010825        frogking            politics      2    2      0  2009-01   \n",
       "\n",
       "                 created_utc  \\\n",
       "0        2016-10-16 23:55:23   \n",
       "1        2016-11-01 00:24:10   \n",
       "2        2016-09-22 21:45:37   \n",
       "3        2016-10-18 21:03:47   \n",
       "4        2016-12-30 17:00:13   \n",
       "...                      ...   \n",
       "1010821  2009-04-25 00:47:52   \n",
       "1010822  2009-05-14 22:27:40   \n",
       "1010823  2009-01-11 00:22:57   \n",
       "1010824  2009-01-23 21:12:49   \n",
       "1010825  2009-01-24 06:20:14   \n",
       "\n",
       "                                            parent_comment  \n",
       "0        Yeah, I get that argument. At this point, I'd ...  \n",
       "1        The blazers and Mavericks (The wests 5 and 6 s...  \n",
       "2                                  They're favored to win.  \n",
       "3                               deadass don't kill my buzz  \n",
       "4        Yep can confirm I saw the tool they use for th...  \n",
       "...                                                    ...  \n",
       "1010821  No one is calling this an engineered pathogen,...  \n",
       "1010822  In a move typical of their recent do-nothing a...  \n",
       "1010823  Screw the Disabled--I've got to get to Church ...  \n",
       "1010824  I've always been unsettled by that. I hear a l...  \n",
       "1010825  Why do the people who make our laws seem unabl...  \n",
       "\n",
       "[1010826 rows x 10 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.dropna(subset=['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "      <td>Trumpbart</td>\n",
       "      <td>politics</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-16 23:55:23</td>\n",
       "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>Shbshb906</td>\n",
       "      <td>nba</td>\n",
       "      <td>-4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>2016-11-01 00:24:10</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>Creepeth</td>\n",
       "      <td>nfl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-22 21:45:37</td>\n",
       "      <td>They're favored to win.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>icebrotha</td>\n",
       "      <td>BlackPeopleTwitter</td>\n",
       "      <td>-8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-18 21:03:47</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>cush2push</td>\n",
       "      <td>MaddenUltimateTeam</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>2016-12-30 17:00:13</td>\n",
       "      <td>Yep can confirm I saw the tool they use for th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010821</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm sure that Iran and N. Korea have the techn...</td>\n",
       "      <td>TwarkMain</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-04</td>\n",
       "      <td>2009-04-25 00:47:52</td>\n",
       "      <td>No one is calling this an engineered pathogen,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010822</th>\n",
       "      <td>1</td>\n",
       "      <td>whatever you do, don't vote green!</td>\n",
       "      <td>BCHarvey</td>\n",
       "      <td>climate</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-05</td>\n",
       "      <td>2009-05-14 22:27:40</td>\n",
       "      <td>In a move typical of their recent do-nothing a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010823</th>\n",
       "      <td>1</td>\n",
       "      <td>Perhaps this is an atheist conspiracy to make ...</td>\n",
       "      <td>rebelcommander</td>\n",
       "      <td>atheism</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-01</td>\n",
       "      <td>2009-01-11 00:22:57</td>\n",
       "      <td>Screw the Disabled--I've got to get to Church ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010824</th>\n",
       "      <td>1</td>\n",
       "      <td>The Slavs got their own country - it is called...</td>\n",
       "      <td>catsi</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-01</td>\n",
       "      <td>2009-01-23 21:12:49</td>\n",
       "      <td>I've always been unsettled by that. I hear a l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010825</th>\n",
       "      <td>1</td>\n",
       "      <td>values, as in capitalism .. there is good mone...</td>\n",
       "      <td>frogking</td>\n",
       "      <td>politics</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-01</td>\n",
       "      <td>2009-01-24 06:20:14</td>\n",
       "      <td>Why do the people who make our laws seem unabl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1010773 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                            comment  \\\n",
       "0            0                                         NC and NH.   \n",
       "1            0  You do know west teams play against west teams...   \n",
       "2            0  They were underdogs earlier today, but since G...   \n",
       "3            0  This meme isn't funny none of the \"new york ni...   \n",
       "4            0                    I could use one of those tools.   \n",
       "...        ...                                                ...   \n",
       "1010821      1  I'm sure that Iran and N. Korea have the techn...   \n",
       "1010822      1                 whatever you do, don't vote green!   \n",
       "1010823      1  Perhaps this is an atheist conspiracy to make ...   \n",
       "1010824      1  The Slavs got their own country - it is called...   \n",
       "1010825      1  values, as in capitalism .. there is good mone...   \n",
       "\n",
       "                 author           subreddit  score  ups  downs     date  \\\n",
       "0             Trumpbart            politics      2   -1     -1  2016-10   \n",
       "1             Shbshb906                 nba     -4   -1     -1  2016-11   \n",
       "2              Creepeth                 nfl      3    3      0  2016-09   \n",
       "3             icebrotha  BlackPeopleTwitter     -8   -1     -1  2016-10   \n",
       "4             cush2push  MaddenUltimateTeam      6   -1     -1  2016-12   \n",
       "...                 ...                 ...    ...  ...    ...      ...   \n",
       "1010821       TwarkMain          reddit.com      2    2      0  2009-04   \n",
       "1010822        BCHarvey             climate      1    1      0  2009-05   \n",
       "1010823  rebelcommander             atheism      1    1      0  2009-01   \n",
       "1010824           catsi           worldnews      1    1      0  2009-01   \n",
       "1010825        frogking            politics      2    2      0  2009-01   \n",
       "\n",
       "                 created_utc  \\\n",
       "0        2016-10-16 23:55:23   \n",
       "1        2016-11-01 00:24:10   \n",
       "2        2016-09-22 21:45:37   \n",
       "3        2016-10-18 21:03:47   \n",
       "4        2016-12-30 17:00:13   \n",
       "...                      ...   \n",
       "1010821  2009-04-25 00:47:52   \n",
       "1010822  2009-05-14 22:27:40   \n",
       "1010823  2009-01-11 00:22:57   \n",
       "1010824  2009-01-23 21:12:49   \n",
       "1010825  2009-01-24 06:20:14   \n",
       "\n",
       "                                            parent_comment  \n",
       "0        Yeah, I get that argument. At this point, I'd ...  \n",
       "1        The blazers and Mavericks (The wests 5 and 6 s...  \n",
       "2                                  They're favored to win.  \n",
       "3                               deadass don't kill my buzz  \n",
       "4        Yep can confirm I saw the tool they use for th...  \n",
       "...                                                    ...  \n",
       "1010821  No one is calling this an engineered pathogen,...  \n",
       "1010822  In a move typical of their recent do-nothing a...  \n",
       "1010823  Screw the Disabled--I've got to get to Church ...  \n",
       "1010824  I've always been unsettled by that. I hear a l...  \n",
       "1010825  Why do the people who make our laws seem unabl...  \n",
       "\n",
       "[1010773 rows x 10 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(comment):\n",
    "    '''This function uses str methods from the string class to remove punctuation from the text.'''\n",
    "    \n",
    "    # replace punctuation with '' (no space)\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    \n",
    "    # return the text stripped of punctuation marks\n",
    "    return comment.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Donghyeok\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH</td>\n",
       "      <td>Trumpbart</td>\n",
       "      <td>politics</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-16 23:55:23</td>\n",
       "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>Shbshb906</td>\n",
       "      <td>nba</td>\n",
       "      <td>-4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>2016-11-01 00:24:10</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today but since Gr...</td>\n",
       "      <td>Creepeth</td>\n",
       "      <td>nfl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-22 21:45:37</td>\n",
       "      <td>They're favored to win.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isnt funny none of the new york nigg...</td>\n",
       "      <td>icebrotha</td>\n",
       "      <td>BlackPeopleTwitter</td>\n",
       "      <td>-8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-18 21:03:47</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools</td>\n",
       "      <td>cush2push</td>\n",
       "      <td>MaddenUltimateTeam</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>2016-12-30 17:00:13</td>\n",
       "      <td>Yep can confirm I saw the tool they use for th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment     author  \\\n",
       "0      0                                          NC and NH  Trumpbart   \n",
       "1      0  You do know west teams play against west teams...  Shbshb906   \n",
       "2      0  They were underdogs earlier today but since Gr...   Creepeth   \n",
       "3      0  This meme isnt funny none of the new york nigg...  icebrotha   \n",
       "4      0                     I could use one of those tools  cush2push   \n",
       "\n",
       "            subreddit  score  ups  downs     date          created_utc  \\\n",
       "0            politics      2   -1     -1  2016-10  2016-10-16 23:55:23   \n",
       "1                 nba     -4   -1     -1  2016-11  2016-11-01 00:24:10   \n",
       "2                 nfl      3    3      0  2016-09  2016-09-22 21:45:37   \n",
       "3  BlackPeopleTwitter     -8   -1     -1  2016-10  2016-10-18 21:03:47   \n",
       "4  MaddenUltimateTeam      6   -1     -1  2016-12  2016-12-30 17:00:13   \n",
       "\n",
       "                                      parent_comment  \n",
       "0  Yeah, I get that argument. At this point, I'd ...  \n",
       "1  The blazers and Mavericks (The wests 5 and 6 s...  \n",
       "2                            They're favored to win.  \n",
       "3                         deadass don't kill my buzz  \n",
       "4  Yep can confirm I saw the tool they use for th...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['comment'] = train['comment'].apply(remove_punc)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you',\n",
       "       \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself',\n",
       "       'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her',\n",
       "       'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them',\n",
       "       'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom',\n",
       "       'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are',\n",
       "       'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\n",
       "       'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and',\n",
       "       'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at',\n",
       "       'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
       "       'through', 'during', 'before', 'after', 'above', 'below', 'to',\n",
       "       'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under',\n",
       "       'again', 'further', 'then', 'once', 'here', 'there', 'when',\n",
       "       'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
       "       'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own',\n",
       "       'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will',\n",
       "       'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll',\n",
       "       'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn',\n",
       "       \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\",\n",
       "       'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma',\n",
       "       'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\",\n",
       "       'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\",\n",
       "       'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
       "      dtype='<U10')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load stopwords from NLTK\n",
    "stopwords_eng = stopwords.words('english')\n",
    "# view stop words\n",
    "np.array(stopwords_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_and_lowercase(comment):\n",
    "    '''This function lowercases words and then remove stopwords.'''\n",
    "    \n",
    "    comment = \\\n",
    "    [word.lower() for word in comment.split() if word.lower() not in stopwords_eng]\n",
    "    return ' '.join(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Donghyeok\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>nc nh</td>\n",
       "      <td>Trumpbart</td>\n",
       "      <td>politics</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-16 23:55:23</td>\n",
       "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>know west teams play west teams east teams right</td>\n",
       "      <td>Shbshb906</td>\n",
       "      <td>nba</td>\n",
       "      <td>-4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>2016-11-01 00:24:10</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>underdogs earlier today since gronks announcem...</td>\n",
       "      <td>Creepeth</td>\n",
       "      <td>nfl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-22 21:45:37</td>\n",
       "      <td>They're favored to win.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>meme isnt funny none new york nigga ones</td>\n",
       "      <td>icebrotha</td>\n",
       "      <td>BlackPeopleTwitter</td>\n",
       "      <td>-8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-18 21:03:47</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>could use one tools</td>\n",
       "      <td>cush2push</td>\n",
       "      <td>MaddenUltimateTeam</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>2016-12-30 17:00:13</td>\n",
       "      <td>Yep can confirm I saw the tool they use for th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment     author  \\\n",
       "0      0                                              nc nh  Trumpbart   \n",
       "1      0   know west teams play west teams east teams right  Shbshb906   \n",
       "2      0  underdogs earlier today since gronks announcem...   Creepeth   \n",
       "3      0           meme isnt funny none new york nigga ones  icebrotha   \n",
       "4      0                                could use one tools  cush2push   \n",
       "\n",
       "            subreddit  score  ups  downs     date          created_utc  \\\n",
       "0            politics      2   -1     -1  2016-10  2016-10-16 23:55:23   \n",
       "1                 nba     -4   -1     -1  2016-11  2016-11-01 00:24:10   \n",
       "2                 nfl      3    3      0  2016-09  2016-09-22 21:45:37   \n",
       "3  BlackPeopleTwitter     -8   -1     -1  2016-10  2016-10-18 21:03:47   \n",
       "4  MaddenUltimateTeam      6   -1     -1  2016-12  2016-12-30 17:00:13   \n",
       "\n",
       "                                      parent_comment  \n",
       "0  Yeah, I get that argument. At this point, I'd ...  \n",
       "1  The blazers and Mavericks (The wests 5 and 6 s...  \n",
       "2                            They're favored to win.  \n",
       "3                         deadass don't kill my buzz  \n",
       "4  Yep can confirm I saw the tool they use for th...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the function to each comment\n",
    "train['comment'] = train['comment'].apply(remove_stopwords_and_lowercase)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def stem_each_word(text):\n",
    "    '''This function stems each word in `text`'''\n",
    "    \n",
    "    text = [stemmer.stem(word) for word in text.split()]\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Donghyeok\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>nc nh</td>\n",
       "      <td>Trumpbart</td>\n",
       "      <td>politics</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-16 23:55:23</td>\n",
       "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>know west team play west team east team right</td>\n",
       "      <td>Shbshb906</td>\n",
       "      <td>nba</td>\n",
       "      <td>-4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>2016-11-01 00:24:10</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>underdog earlier today sinc gronk announc afte...</td>\n",
       "      <td>Creepeth</td>\n",
       "      <td>nfl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-22 21:45:37</td>\n",
       "      <td>They're favored to win.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>meme isnt funni none new york nigga one</td>\n",
       "      <td>icebrotha</td>\n",
       "      <td>BlackPeopleTwitter</td>\n",
       "      <td>-8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-18 21:03:47</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>could use one tool</td>\n",
       "      <td>cush2push</td>\n",
       "      <td>MaddenUltimateTeam</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>2016-12-30 17:00:13</td>\n",
       "      <td>Yep can confirm I saw the tool they use for th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment     author  \\\n",
       "0      0                                              nc nh  Trumpbart   \n",
       "1      0      know west team play west team east team right  Shbshb906   \n",
       "2      0  underdog earlier today sinc gronk announc afte...   Creepeth   \n",
       "3      0            meme isnt funni none new york nigga one  icebrotha   \n",
       "4      0                                 could use one tool  cush2push   \n",
       "\n",
       "            subreddit  score  ups  downs     date          created_utc  \\\n",
       "0            politics      2   -1     -1  2016-10  2016-10-16 23:55:23   \n",
       "1                 nba     -4   -1     -1  2016-11  2016-11-01 00:24:10   \n",
       "2                 nfl      3    3      0  2016-09  2016-09-22 21:45:37   \n",
       "3  BlackPeopleTwitter     -8   -1     -1  2016-10  2016-10-18 21:03:47   \n",
       "4  MaddenUltimateTeam      6   -1     -1  2016-12  2016-12-30 17:00:13   \n",
       "\n",
       "                                      parent_comment  \n",
       "0  Yeah, I get that argument. At this point, I'd ...  \n",
       "1  The blazers and Mavericks (The wests 5 and 6 s...  \n",
       "2                            They're favored to win.  \n",
       "3                         deadass don't kill my buzz  \n",
       "4  Yep can confirm I saw the tool they use for th...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply stem_each_word function to each TRAIN and TEST comment\n",
    "\n",
    "# TRAIN\n",
    "train['comment'] = train['comment'].apply(stem_each_word)\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_each_word(text):\n",
    "    '''This function lemmatizes each word, e.g., \n",
    "    - Remove the final \"s\" or \"es\" to singularize plurals and change person of some verbs\n",
    "      (gets --> get,\n",
    "      passes --> pass, \n",
    "      BUT not always as expected: \n",
    "      does --> doe (a deer, a female deer), \n",
    "      capitalizes --> captializes)\n",
    "    '''\n",
    "    text = [lemmatizer.lemmatize(word) for word in text.split()]\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Donghyeok\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>nc nh</td>\n",
       "      <td>Trumpbart</td>\n",
       "      <td>politics</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-16 23:55:23</td>\n",
       "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>know west team play west team east team right</td>\n",
       "      <td>Shbshb906</td>\n",
       "      <td>nba</td>\n",
       "      <td>-4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>2016-11-01 00:24:10</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>underdog earlier today sinc gronk announc afte...</td>\n",
       "      <td>Creepeth</td>\n",
       "      <td>nfl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-22 21:45:37</td>\n",
       "      <td>They're favored to win.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>meme isnt funni none new york nigga one</td>\n",
       "      <td>icebrotha</td>\n",
       "      <td>BlackPeopleTwitter</td>\n",
       "      <td>-8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-18 21:03:47</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>could use one tool</td>\n",
       "      <td>cush2push</td>\n",
       "      <td>MaddenUltimateTeam</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>2016-12-30 17:00:13</td>\n",
       "      <td>Yep can confirm I saw the tool they use for th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment     author  \\\n",
       "0      0                                              nc nh  Trumpbart   \n",
       "1      0      know west team play west team east team right  Shbshb906   \n",
       "2      0  underdog earlier today sinc gronk announc afte...   Creepeth   \n",
       "3      0            meme isnt funni none new york nigga one  icebrotha   \n",
       "4      0                                 could use one tool  cush2push   \n",
       "\n",
       "            subreddit  score  ups  downs     date          created_utc  \\\n",
       "0            politics      2   -1     -1  2016-10  2016-10-16 23:55:23   \n",
       "1                 nba     -4   -1     -1  2016-11  2016-11-01 00:24:10   \n",
       "2                 nfl      3    3      0  2016-09  2016-09-22 21:45:37   \n",
       "3  BlackPeopleTwitter     -8   -1     -1  2016-10  2016-10-18 21:03:47   \n",
       "4  MaddenUltimateTeam      6   -1     -1  2016-12  2016-12-30 17:00:13   \n",
       "\n",
       "                                      parent_comment  \n",
       "0  Yeah, I get that argument. At this point, I'd ...  \n",
       "1  The blazers and Mavericks (The wests 5 and 6 s...  \n",
       "2                            They're favored to win.  \n",
       "3                         deadass don't kill my buzz  \n",
       "4  Yep can confirm I saw the tool they use for th...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply lemmatize_each_word(text) function to each TRAIN and TEST comment\n",
    "\n",
    "# TRAIN\n",
    "train['comment'] = train['comment'].apply(lemmatize_each_word)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=50000, min_df=2)\n",
    "train_comments = vectorizer.fit_transform(train['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = \n",
    "labels = df['label']\n",
    "parent_comments = df['parent_comment']\n",
    "subreddit = df['subreddit']\n",
    "score = df['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(lines):\n",
    "    return max([len(s.split()) for s in lines])\n",
    "\n",
    "def pad_text(tokenizer, lines, length):\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post', truncating='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample_df_idx1 = np.random.choice(df1.loc[df1['label']==0].index, int(len(df1.loc[df1['label']==0].index)))\n",
    "sample_df_idx2 = np.random.choice(df1.loc[df1['label']==1].index, int(len(df1.loc[df1['label']==1].index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_df_idx = np.concatenate((sample_df_idx1, sample_df_idx2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample_df = df1.loc[sample_df_idx]\n",
    "sample_comments = comments.loc[sample_df_idx]\n",
    "sample_labels = labels.loc[sample_df_idx]\n",
    "sample_parent_comments = parent_comments.loc[sample_df_idx]\n",
    "sample_subreddit = subreddit.loc[sample_df_idx]\n",
    "sample_score = score.loc[sample_df_idx]\n",
    "sample_ups = ups.loc[sample_df_idx]\n",
    "sample_downs = downs.loc[sample_df_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "vocab_size = 8000\n",
    "\n",
    "tokenized_comments = tokenize_text(sample_comments, vocab_size)\n",
    "length = max_length(sample_comments)\n",
    "main_vocab_size = len(tokenized_comments.word_index) + 1\n",
    "print('Max document length: %d' % length)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "\n",
    "tokenized_parent_comments = tokenize_text(sample_parent_comments, vocab_size)\n",
    "parent_length = max_length(parent_comments)\n",
    "parent_vocab_size = len(tokenized_parent_comments.word_index) + 1\n",
    "print('PC Max document length: %d' % parent_length)\n",
    "print('PC Vocabulary size: %d' % parent_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample_comments[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"sequences:\", tokenized_comments.texts_to_sequences(sample_comments[2]))\n",
    "print(\"word index:\", tokenized_comments.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mask = np.random.rand(len(sample_df)) < 0.8\n",
    "train_data = sample_comments[mask]\n",
    "train_labels = sample_labels[mask]\n",
    "val_data = sample_comments[~mask]\n",
    "val_labels = sample_labels[~mask]\n",
    "\n",
    "train_padded = pad_text(tokenized_comments, train_data, length)\n",
    "val_padded = pad_text(tokenized_comments, val_data, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded = np.array(train_padded)\n",
    "val_padded = np.array(val_padded)\n",
    "print(train_padded.shape)\n",
    "print(val_padded.shape)\n",
    "\n",
    "#labels processing\n",
    "train_labels = np.array(train_labels)\n",
    "val_labels = np.array(val_labels)\n",
    "print(train_labels.shape)\n",
    "print(val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_padded, train_labels))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_padded, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "def configure_dataset(dataset):\n",
    "    return dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = configure_dataset(train_dataset)\n",
    "val_dataset = configure_dataset(val_dataset)\n",
    "#test_dataset = configure_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "# embedding_layer = Embedding(len(word_index)+1, EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "#                                                input_length=MAX_SEQUENCE_LENGTH, trainable=False)\n",
    "\n",
    "# x = embedding_layer(sequence_input)\n",
    "# x = Dropout(0.3)(x)\n",
    "# x = LSTM(100)(x)\n",
    "# prob = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# model = Model(sequence_input, prob)\n",
    "# model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Embedding(vocab_size, embedding_dim, input_length=length))\n",
    "# model.add(LSTM(256))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size+1, embedding_dim,input_length = train_padded.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(196, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "hisotry = model.fit(train_padded, train_labels, validation_data=(val_padded, val_labels), batch_size=32, epochs=num_epochs, verbose=2, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lstm = model.fit(train_dataset, validation_data=val_dataset, epochs=num_epochs, callbacks=[es])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size + 1, embedding_dim, input_length=length))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "hisotry = model.fit(train_dataset, validation_data=val_dataset, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(hisotry.history['loss'])\n",
    "plt.plot(hisotry.history['val_loss'])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()\n",
    "  \n",
    "plt.plot(hisotry.history['accuracy'])\n",
    "plt.plot(hisotry.history['val_accuracy'])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['accuracy', 'val_accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lstm.history['loss'])\n",
    "plt.plot(lstm.history['val_loss'])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()\n",
    "  \n",
    "plt.plot(lstm.history['accuracy'])\n",
    "plt.plot(lstm.history['val_accuracy'])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['accuracy', 'val_accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predictions > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(test_labels, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = matrix.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_cm = pd.DataFrame(matrix, index = [\"Actual Negative\", \"Actual Positive\"],\n",
    "                      columns = [\"Predicted Negative\", \"Predicted Positive\"])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True, fmt='d', cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifer = MultinomialNB().fit(X_train_tfidf, data_train.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(C=1, n_jobs=4, solver='lbfgs', \n",
    "                           random_state=17, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
