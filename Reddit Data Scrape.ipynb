{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def get_pushshift_data(data_type, **kwargs):\n",
    "    \"\"\"\n",
    "    Gets data from the pushshift api.\n",
    " \n",
    "    data_type can be 'comment' or 'submission'\n",
    "    The rest of the args are interpreted as payload.\n",
    " \n",
    "    Read more: https://github.com/pushshift/api\n",
    "    \"\"\"\n",
    " \n",
    "    base_url = f\"https://files.pushshift.io/reddit/comments/\"\n",
    "    payload = kwargs\n",
    "    request = requests.get(base_url, params=payload)\n",
    "    return request.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'httplib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-12a87fb7e292>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mhttplib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mhttplib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHTTPConnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_http_vsn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mhttplib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHTTPConnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_http_vsn_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'HTTP/1.0'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'httplib'"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import httplib\n",
    "httplib.HTTPConnection._http_vsn = 10\n",
    "httplib.HTTPConnection._http_vsn_str = 'HTTP/1.0'\n",
    "\n",
    "prefix = 'c'\n",
    "pattern_filter = \"*2018*&*2019*\"\n",
    "download_filter = list([x.strip() for x in pattern_filter.split('&')])\n",
    "download_filter.sort()\n",
    "\n",
    "keyword_filter = ''\n",
    "present_download_files = 'Downloaded->           '       \n",
    "keyword_filter = 'rc'\n",
    "present_download_files = 'RC ' + present_download_files\n",
    "r_df  = requests.get(\"https://files.pushshift.io/reddit/comments/\")\n",
    "data_df = r_df.text\n",
    "soup_df = BeautifulSoup(data_df,features=\"lxml\")\n",
    "\n",
    "#Get the href links from the website\n",
    "list_of_href_df = set()\n",
    "for link_df in soup_df.find_all('a'):\n",
    "    href_link_df = link_df.get('href')\n",
    "    list_of_href_df.add(\"https://files.pushshift.io/reddit/comments/\" + href_link_df[2:])\n",
    "\n",
    "#Clean from external href links     \n",
    "clean_list_of_href_df = []\n",
    "for loh_df in list_of_href_df:\n",
    "    if keyword_filter in loh_df.lower():\n",
    "        clean_list_of_href_df.append(loh_df)\n",
    "        \n",
    "#Filter the links\n",
    "matching_fnmatch_list = []\n",
    "if pattern_filter != '':\n",
    "    if prefix == '*':\n",
    "        pass\n",
    "    for dfilter in download_filter:\n",
    "        fnmatch_list = fnmatch.filter(clean_list_of_href_df, dfilter) \n",
    "        i = 0\n",
    "        for fnl in fnmatch_list:\n",
    "            '''temporary break to test'''\n",
    "            if i == 2:\n",
    "                break\n",
    "            i += 1\n",
    "            matching_fnmatch_list.append(fnl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://files.pushshift.io/reddit/comments/RC_2018-10.xz',\n",
       " 'https://files.pushshift.io/reddit/comments/RC_2018-12.zst',\n",
       " 'https://files.pushshift.io/reddit/comments/RC_2019-10.zst',\n",
       " 'https://files.pushshift.io/reddit/comments/RC_2019-05.zst']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_fnmatch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'https://files.pushshift.io/reddit/comments/RC_2019-01.zst',\n",
    "'https://files.pushshift.io/reddit/comments/RC_2019-02.zst',\n",
    "'https://files.pushshift.io/reddit/comments/RC_2019-03.zst',\n",
    "'https://files.pushshift.io/reddit/comments/RC_2019-04.zst',\n",
    "'https://files.pushshift.io/reddit/comments/RC_2019-05.zst',\n",
    "'https://files.pushshift.io/reddit/comments/RC_2019-06.zst',\n",
    "'https://files.pushshift.io/reddit/comments/RC_2019-07.zst',\n",
    "'https://files.pushshift.io/reddit/comments/RC_2019-08.zst',\n",
    "'https://files.pushshift.io/reddit/comments/RC_2019-09.zst',\n",
    "'https://files.pushshift.io/reddit/comments/RC_2019-10.zst',\n",
    "'https://files.pushshift.io/reddit/comments/RC_2019-11.zst',\n",
    "'https://files.pushshift.io/reddit/comments/RC_2019-12.zst'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IncompleteRead",
     "evalue": "IncompleteRead(507575 bytes read, 15907980594 more expected)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIncompleteRead\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-63eddcaf1659>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mrequest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://files.pushshift.io/reddit/comments/RC_2019-11.zst'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdata_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_save_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mwf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mwf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    468\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 470\u001b[1;33m                     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    471\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_close_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    620\u001b[0m             \u001b[0mchunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAXAMOUNT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 622\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    623\u001b[0m             \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[0mamt\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIncompleteRead\u001b[0m: IncompleteRead(507575 bytes read, 15907980594 more expected)"
     ]
    }
   ],
   "source": [
    "path_to_save_filename = 'C:/Users/Donghyeok/Desktop/Work Folder/Reddit Sarcasm Detection/reddit_pushshift-master/'\n",
    "request = urllib.request.Request('https://files.pushshift.io/reddit/comments/RC_2019-12.zst')\n",
    "response = urllib.request.urlopen(request)\n",
    "data_content = response.read()\n",
    "with open(path_to_save_filename, 'wb') as wf:    \n",
    "    wf.write(data_content)                 \n",
    "    print('Downloaded    ->    ' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "httplib.HTTPConnection._http_vsn = 11\n",
    "httplib.HTTPConnection._http_vsn_str = 'HTTP/1.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import fnmatch\n",
    "import requests\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "import bz2\n",
    "import lzma\n",
    "import gzip\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import zstandard as zstd\n",
    "import multiprocessing as mp\n",
    "\n",
    "class reddit:\n",
    "    def multithread_download_files_func(self,list_of_file):\n",
    "        filename = list_of_file[list_of_file.rfind(\"/\")+1:]\n",
    "        path_to_save_filename = self.ptsf_download_files + filename\n",
    "        if not os.path.exists(path_to_save_filename): \n",
    "            data_content = None\n",
    "            try:\n",
    "                request = urllib.request.Request(list_of_file)\n",
    "                response = urllib.request.urlopen(request)\n",
    "                data_content = response.read()\n",
    "            except urllib.error.HTTPError:\n",
    "                retries = 1\n",
    "                success = False\n",
    "                while not success:\n",
    "                    try:\n",
    "                        response = urllib.request.urlopen(list_of_file)\n",
    "                        data_content = response.read()                        \n",
    "                        success = True\n",
    "                    except Exception:\n",
    "                        wait = retries * 15;\n",
    "                        print('Service Temporarily Unavailable! Retrying in ' + str(wait) + 's on file: ' + filename)\n",
    "                        sys.stdout.flush()                          \n",
    "                        time.sleep(wait)\n",
    "                        retries += 1               \n",
    "            if data_content:\n",
    "                with open(path_to_save_filename, 'wb') as wf:    \n",
    "                    wf.write(data_content)                 \n",
    "                    print(self.present_download_files + filename)\n",
    "    def download_files(self,filter_files_df,url_to_download_df,path_to_save_file_df,prefix):\n",
    "        if filter_files_df == '*':\n",
    "            reddit_years = [2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018]\n",
    "            filter_files_df = ''\n",
    "            for idx, ry in enumerate(reddit_years):\n",
    "                filter_files_df += '*' + str(ry) + '*'\n",
    "                if (idx != len(reddit_years)-1):\n",
    "                    filter_files_df += '&'   \n",
    "\n",
    "        download_filter = list([x.strip() for x in filter_files_df.split('&')])\n",
    "        download_filter.sort()\n",
    "\n",
    "        keyword_filter = ''\n",
    "        self.present_download_files = 'Downloaded->           '\n",
    "        if prefix == 's':\n",
    "            self.ptsf_download_files = path_to_save_file_df + 'Step 1 - Data Lake\\Reddit Submissions\\\\'\n",
    "            keyword_filter = 'rs'\n",
    "            self.present_download_files = 'RS ' + self.present_download_files\n",
    "            #Include 1 previous year for submissions only\n",
    "            #min_year = download_filter[0] \n",
    "            #int_min_year = int(min_year[1:-1])-1 #minus 1 year            \n",
    "            #download_filter.append('*' + str(int_min_year) + '*')  \n",
    "            #download_filter.sort()\n",
    "\n",
    "        if prefix == 'c':\n",
    "            self.ptsf_download_files = path_to_save_file_df + 'Step 1 - Data Lake\\Reddit Comments\\\\'        \n",
    "            keyword_filter = 'rc'\n",
    "            self.present_download_files = 'RC ' + self.present_download_files\n",
    "\n",
    "        #If folder doesn't exist, create one\n",
    "        if not os.path.exists(os.path.dirname(self.ptsf_download_files)):\n",
    "            os.makedirs(os.path.dirname(self.ptsf_download_files))        \n",
    "\n",
    "        r_df  = requests.get(url_to_download_df)\n",
    "        data_df = r_df.text\n",
    "        soup_df = BeautifulSoup(data_df,features=\"lxml\")\n",
    "\n",
    "        #Get the href links from the website\n",
    "        list_of_href_df = set()\n",
    "        for link_df in soup_df.find_all('a'):\n",
    "            href_link_df = link_df.get('href')\n",
    "            list_of_href_df.add(url_to_download_df + href_link_df[2:])\n",
    "\n",
    "        #Clean from external href links     \n",
    "        clean_list_of_href_df = []\n",
    "        for loh_df in list_of_href_df:\n",
    "            if keyword_filter in loh_df.lower():\n",
    "                clean_list_of_href_df.append(loh_df)\n",
    "\n",
    "        #Filter the links\n",
    "        matching_fnmatch_list = []\n",
    "        if filter_files_df != '':\n",
    "            if prefix == '*':\n",
    "                pass\n",
    "            for dfilter in download_filter:\n",
    "                fnmatch_list = fnmatch.filter(clean_list_of_href_df, dfilter) \n",
    "                i = 0\n",
    "                for fnl in fnmatch_list:\n",
    "                    '''temporary break to test'''\n",
    "                    if i == 2:\n",
    "                        break\n",
    "                    i += 1\n",
    "                    matching_fnmatch_list.append(fnl)\n",
    "\n",
    "        if not matching_fnmatch_list:\n",
    "            matching_fnmatch_list = clean_list_of_href_df.sort()\n",
    "        matching_fnmatch_list.sort()\n",
    "\n",
    "        p = ThreadPool(200)\n",
    "        p.map(self.multithread_download_files_func, matching_fnmatch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decompress:\n",
    "        \n",
    "    def multiprocess_extaction_func(self,path_to_read_file):\n",
    "         file_name = path_to_read_file[path_to_read_file.rfind(\"\\\\\")+1:]\n",
    "         file_name_extension = Path(file_name).suffix\n",
    "         file_name_wo_extension = file_name[:file_name.rfind(file_name_extension)]\n",
    "         path_to_save_file = self.path_to_save_file_df + file_name_wo_extension\n",
    "         if file_name_extension == \".bz2\":\n",
    "             with bz2.BZ2File(path_to_read_file,\"rb\") as fr, open(path_to_save_file,\"wb\") as fw:\n",
    "                 shutil.copyfileobj(fr,fw,length = 65536)\n",
    "                 print(self.present_decompress_file + file_name_wo_extension)\n",
    "         elif file_name_extension == \".xz\":\n",
    "             with lzma.open(path_to_read_file, 'rb') as fr, open(path_to_save_file, 'wb') as fw:\n",
    "                 shutil.copyfileobj(fr, fw, 65536)\n",
    "                 print(self.present_decompress_file + file_name_wo_extension)              \n",
    "         elif file_name_extension == \".gz\": \n",
    "             with gzip.open(path_to_read_file, 'rb') as fr, open(path_to_save_file, 'wb') as fw:\n",
    "                 shutil.copyfileobj(fr, fw, 65536)\n",
    "                 print(self.present_decompress_file + file_name_wo_extension)             \n",
    "         elif file_name_extension == \".zst\":\n",
    "             dctx = zstd.ZstdDecompressor()\n",
    "             with open(path_to_read_file, 'rb') as ifh, open(path_to_save_file, 'wb') as ofh:\n",
    "                dctx.copy_stream(ifh, ofh, write_size=65536) \n",
    "                print(self.present_decompress_file + file_name_wo_extension)               \n",
    "     \n",
    "    def decompress_file(self,dir_to_read_file,prefix):\n",
    "        self.present_decompress_file = 'Decompressed->         '  \n",
    "        if prefix == 's':\n",
    "            self.present_decompress_file = 'RS ' + self.present_decompress_file\n",
    "            self.dtrf_df = dir_to_read_file + 'Step 1 - Data Lake\\Reddit Submissions\\\\'\n",
    "            self.path_to_save_file_df = dir_to_read_file + 'Step 2 - Decompress\\Reddit Submissions\\\\'  \n",
    "        if prefix == 'c':\n",
    "            self.present_decompress_file = 'RC ' + self.present_decompress_file\n",
    "            self.dtrf_df = dir_to_read_file + 'Step 1 - Data Lake\\Reddit Comments\\\\'\n",
    "            self.path_to_save_file_df = dir_to_read_file + 'Step 2 - Decompress\\Reddit Comments\\\\'   \n",
    "        if prefix == 'w':\n",
    "            self.present_decompress_file = 'WK ' + self.present_decompress_file\n",
    "            self.dtrf_df = dir_to_read_file + 'Step 1 - Data Lake\\Wikipedia Pagecounts\\\\'\n",
    "            self.path_to_save_file_df = dir_to_read_file + 'Step 2 - Decompress\\Wikipedia Pagecounts\\\\'  \n",
    "        extraction_list = []           \n",
    "        directory = os.fsencode(self.dtrf_df)\n",
    "        if not os.path.exists(os.path.dirname(self.path_to_save_file_df)):\n",
    "            os.makedirs(os.path.dirname(self.path_to_save_file_df))                \n",
    "        \n",
    "        for file_to_uncompress in os.listdir(directory):\n",
    "            extraction_list.append(directory.decode() + file_to_uncompress.decode())  \n",
    "\n",
    "        extraction_list.sort()    \n",
    "        with mp.Pool(mp.cpu_count()) as p:\n",
    "            p.map(self.multiprocess_extaction_func, extraction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  Reddit Comments\n"
     ]
    },
    {
     "ename": "IncompleteRead",
     "evalue": "IncompleteRead(392196 bytes read, 14693529258 more expected)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIncompleteRead\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-d89a6e3abd40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdir_of_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'C:/Users/Donghyeok/Desktop/Work Folder/Reddit Sarcasm Detection/reddit_pushshift-master/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'---  Reddit Comments'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mreddit_repo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern_filter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"https://files.pushshift.io/reddit/comments/\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdir_of_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-3fa338aa7d65>\u001b[0m in \u001b[0;36mdownload_files\u001b[1;34m(self, filter_files_df, url_to_download_df, path_to_save_file_df, prefix)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mThreadPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultithread_download_files_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatching_fnmatch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         '''\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    655\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    656\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 657\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    659\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mworker\u001b[1;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[0mjob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mmapstar\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mstarmapstar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-3fa338aa7d65>\u001b[0m in \u001b[0;36mmultithread_download_files_func\u001b[1;34m(self, list_of_file)\u001b[0m\n\u001b[0;32m     25\u001b[0m                 \u001b[0mrequest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_of_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[0mdata_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHTTPError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                 \u001b[0mretries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    468\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 470\u001b[1;33m                     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    471\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_close_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    620\u001b[0m             \u001b[0mchunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAXAMOUNT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 622\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    623\u001b[0m             \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[0mamt\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIncompleteRead\u001b[0m: IncompleteRead(392196 bytes read, 14693529258 more expected)"
     ]
    }
   ],
   "source": [
    "pattern_filter = \"*2019*\"\n",
    "reddit_repo = reddit()\n",
    "dir_of_file = 'C:/Users/Donghyeok/Desktop/Work Folder/Reddit Sarcasm Detection/reddit_pushshift-master/'\n",
    "print('---  Reddit Comments')\n",
    "reddit_repo.download_files(pattern_filter,\"https://files.pushshift.io/reddit/comments/\",dir_of_file,'c')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decompress_repo = decompress()\n",
    "print('---  Reddit Comments')\n",
    "decompress_repo.decompress_file(dir_of_file,'c')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
